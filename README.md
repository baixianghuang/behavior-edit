# Agent Ethical Behavior Editing

**Repository Overview**: This repository contains the code, data, and experimental results for the paper *"Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior Toward Beneficence or Harm."*

**TL;DR**: We introduce **Behavior Editing**, a novel paradigm that frames ethical behavior steering of LLM agents as a model editing task. Using our benchmark **BehaviorBench**, we demonstrate that model editing can precisely and effectively steer both benevolent and harmful behaviors, underscoring dual-use concerns in model safety and alignment.

**Authors**: [Baixiang Huang\*](https://baixianghuang.github.io/), [Zhen Tan](https://zhen-tan-dmml.github.io/), [Haoran Wang](https://haoranwang18.github.io/), [Zijie Liu](https://www.linkedin.com/in/zijie-liu-186a05208), [Dawei Li](https://david-li0406.github.io/), [Ali Payani](https://www.linkedin.com/in/ali-payani-59267515/), [Huan Liu](http://www.public.asu.edu/~huanliu), [Tianlong Chen](https://tianlong-chen.github.io/), [Kai Shu](https://www.cs.emory.edu/~kshu5/)
- **Paper** : [Read our paper]()
- **Project Website**: Visit [https://llm-editing.github.io](https://llm-editing.github.io/) for more resources.

---

## Overview

As Large Language Model (LLM) agents are deployed in high-stakes, real-world applications, ensuring ethical behavior becomes essential. In this work, we propose **Behavior Editing**, a new framework that formulates ethical behavior steering as a **localized model editing** problem. 

To support rigorous evaluation, we introduce **BehaviorBench**, a multi-layered benchmark grounded in moral psychology, capable of measuring both **scenario-specific edits** and **general moral alignment** shifts. We find that Behavior Editing methods can systematically steer agents toward both prosocial and antisocial behaviors—raising critical safety and dual-use concerns.

Key insights include:
- **Parameter-modifying** approaches outperform **parameter-preserving** ones in editing success.
- Newer models with stronger reasoning capacities are more **resilient to unethical manipulations**.
- Behavior Editing reveals gaps in current safety protocols and emphasizes the need for proactive defense against misuse.

Disclaimer: This repository contains responses generated by agents that are unethical or offensive. These do not reflect the opinions of the authors. Please use the data responsibly.

<img src="./data/intro.jpg" width=100%>


## Table of Contents

1. [Overview](#overview)  
2. [Repository Structure](#repository-structure)  
3. [Installation](#installation)  
4. [Usage](#usage)  
5. [Data Preparation](#data-preparation)  
6. [Running Experiments](#running-experiments)  
7. [Acknowledgements](#acknowledgements)  
8. [Citation](#citation)  

---

## Repository Structure
- `data/`: Contains the behavior evaluation dataset.
- `code/`: Includes scripts and code to evaluate behavior editing using model editing methods (and reproduce the results in the paper).
- `results/`: Results of the experiments that we report in the paper.


## Installation
To set up the environment for running the code, follow these steps:

1. Clone the repository:
    ```bash
    git clone https://github.com/baixianghuang/behavior-edit.git
    cd behavior-edit
    ```

2. Create a virtual environment and activate it:
    ```bash
    conda create -n behavior-edit python=3.9
    conda activate behavior-edit
    ```

3. Install the required dependencies:
    ```bash
    pip install -r requirements.txt
    ```

## Usage

### Data Preparation

1. Datasets are stored in the `data/` directory. There are three folders: 
```bash
data/
    .
    ├── ethics
    ├── general_capability
    ├── machine_ethics
    ├── jiminy_sub_100.json
    ├── jiminy_subset.csv
    ├── jiminy_test.json
    ├── moralchoice_high_ambiguity_101.json
    ├── moralchoice_low_ambiguity_100.json
    └── socialchemistry_morality_ethics_100.json
```
<!-- `ethics` contains the ETHICS dataset, including five moral dimensions. `general_capability` contains data to evaluate general capability before and after model editing. -->

### Running Experiments

**Run example**: To get started (e.g. using ROME to edit llama3-8b on the places_landmark data), run:

```bash
cd ./code
python3 edit_all_method.py \
    --model_name=llama3-8b \
    --edit_method=ROME \
    --topic_name=places_landmark \
    --device_edit=0 \
    --device_eval=1 \
    --data_size=5 \
    --results_dir=../new_results_dir \
    --question_types rephrase_questions questions_2hop
```

Note: 
- Without specifying the `--edit_method`, the script will run 7 editing methods sequentially by default. 
- Specify `--question_types` to choose specific types of questions in the evaluation (The example above will only evalute 2-hop questions and rephrased questions). Otherwise, the script will run all the question types (yes_questions, no_questions, locality_questions, rephrase_questions, multiple_choice_questions, reversed_relation_questions, questions_2hop, questions_3hop, questions_4hop, questions_5hop, questions_6hop). The original questions is always included.
- Specify `--results_dir` to save the results to a specific directory, otherwise the default directory is where we save the results that we report in the paper. You can also use `--overwrite_result` to overwrite the existing result file.
<!-- If you use an API model (such as GPT-4) as the evaluator, you need to set your `YOUR_API_KEY` in Line 60 of `code/editor_new_eval.py`. One example is as follows: -->

To run the multi-turn editing, here is an example:
```bash
python3 edit_all_method_multi_turn.py \
    --model_name=llama3-8b \
    --edit_method=ROME \
    --topic_name=places_landmark \
    --device_edit=0 \
    --device_eval=1 \
    --model_eval=meta-llama/Meta-Llama-3-8B-Instruct \
    --data_size=5 \
    --results_dir=../new_results_dir \
    --multi_turn=yes \
    --multi_turn_num=10
```
- Use `--multi_turn` to choose the type of multi-turn evaluation (`yes` or `sure`).
- Use `--multi_turn_num` to set the number of turns for multi-turn evaluation.


We use a local LLM (e.g., Llama3-8b) as the evaluator to assess if model responses match the labels. For experiments, we recommend using at least one GPU with 48 GB of memory (e.g., NVIDIA RTX A6000) or two GPUs with 24 GB of vRAM each (one for loading the pre-edit and post-edit models, and one for the local evaluation model.) Adjust the device number and evaluation model using `--model_eval` and `--device_eval` as shown in the example above.

For full experiments to reproduce the results in the paper:
1. Experiment for all the 26 topics:
    ```bash
    ./edit_all_topic.sh
    ```

2. Experiment for the robustness evaluation:
    ```bash
    ./code/edit_all_topic_multi_turn.sh
    ```



We evaluate instruction-tuned models including `Llama-2-7B-chat`, `Llama-3-8B-Instruct`, and `Mistral-7B-v0.3`. All parameters are in the `code/hparams/<method_name>/<model_name>`. 

Results are stored at `llama_2_7b_chat_hf`, `meta_llama_3_8b_instruct`, `mistral_7b_instruct_v0.3` under the `results` folder.

To summarize the results, use the jupyter notebook `code/result_table.ipynb`



## Acknowledgements
We gratefully acknowledge the use of code and data from the following projects: [GRACE](https://github.com/thartvigsen/grace), [EasyEdit](https://github.com/zjunlp/EasyEdit), [ROME](https://github.com/kmeng01/rome), [MEMIT](https://github.com/kmeng01/memit)


## Citation
```
@inproceedings{huang2025,

}
```
