# Agent Ethical Behavior Editing
**Repository Overview**: This repository contains the code, data, and experimental results for the paper *"Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior Toward Beneficence or Harm."*

**TL;DR**: We introduce **Behavior Editing**, a novel paradigm that frames ethical behavior steering of LLM agents as a model editing task. Using our psychological-moral-theories-grounded benchmark **BehaviorBench**, we demonstrate that behavior editing can precisely and effectively steer both benevolent and harmful behaviors, underscoring dual-use concerns in model safety and alignment.

**Authors**: [Baixiang Huang\*](https://baixianghuang.github.io/), [Zhen Tan](https://zhen-tan-dmml.github.io/), [Haoran Wang](https://haoranwang18.github.io/), [Zijie Liu](https://www.linkedin.com/in/zijie-liu-186a05208), [Dawei Li](https://david-li0406.github.io/), [Ali Payani](https://www.linkedin.com/in/ali-payani-59267515/), [Huan Liu](http://www.public.asu.edu/~huanliu), [Tianlong Chen](https://tianlong-chen.github.io/), [Kai Shu](https://www.cs.emory.edu/~kshu5/)
- **Paper** : [Read our paper]()
- **Project Website**: Visit [https://llm-editing.github.io](https://llm-editing.github.io/) for more resources.


## Overview
This repository introduces Behavior Editing, a novel paradigm for steering the ethical behavior of large language model (LLM)-based agents through model editing. Rather than fine-tuning or external prompting, Behavior Editing enables direct, efficient, and directional changes to model behavior while preserving general capabilities.

Key Features
- Behavior Editing: A framework for modifying LLM behavior by editing internal model representations, allowing for fine-grained moral steering—either toward benevolent or harmful outcomes.
- BehaviorBench: A multi-tier benchmark grounded in psychological theories of morality, designed to evaluate and compare model editing methods across simple to complex ethical scenarios.
- Moral Alignment Control: Demonstrates the ability to induce broad shifts in agents' ethical alignment beyond isolated cases.

Warning: This repository contains responses generated by agents that are unethical or offensive. These do not reflect the opinions of the authors. Please use the data responsibly.

<img src="./data/intro.jpg" width=100%>


## Table of Contents

1. [Overview](#overview)  
2. [Repository Structure](#repository-structure)  
3. [Installation](#installation)  
4. [Usage](#usage)  
    - [Data Preparation](#data-preparation)
    - [Running Experiments](#running-experiments) 
5. [Citation](#citation)  


## Repository Structure
- `data/`: Contains the behavior evaluation dataset.
- `code/`: Includes scripts and code to evaluate behavior editing using model editing methods (and reproduce the results in the paper).
- `results/`: Results of the experiments that we report in the paper.


## Installation
To set up the environment for running the code, follow these steps:

1. Clone the repository:
    ```bash
    git clone https://github.com/baixianghuang/behavior-edit.git
    cd behavior-edit
    ```

2. Create a virtual environment and activate it:
    ```bash
    conda create -n behavior-edit python=3.9
    conda activate behavior-edit
    ```

3. Install the required dependencies:
    ```bash
    pip install -r requirements.txt
    ```


## Usage

### Data Preparation

1. Datasets are stored in the `data/` directory. There are three folders: 
```bash
data/
    .
    ├── ethics
    ├── general_capability
    ├── moralchoice_high_ambiguity_101.json
    ├── moralchoice_low_ambiguity_100.json
    └── socialchemistry_morality_ethics_100.json
```
- `ethics` contains the ETHICS dataset including five moral dimensions. Data source: [https://github.com/hendrycks/ethics](https://github.com/hendrycks/ethics).
- `general_capability` contains data to evaluate general capability before and after model editing. Data sources: [GSM8K](https://github.com/openai/gsm8k), [BoolQ](https://github.com/google-research-datasets/boolean-questions), [NLI](https://github.com/hendrycks/nli), [Natural Questions](https://github.com/google-research-datasets/natural-questions).
<!-- - `machine_ethics` contains the machine ethics dataset. Data source: [https://github.com/AI-Secure/DecodingTrust](https://github.com/AI-Secure/DecodingTrust). -->
- Jiminy Criket dataset is download from [Hugging Face](https://huggingface.co/datasets/AI-Secure/DecodingTrust/tree/main/machine_ethics).
- `moralchoice_high_ambiguity_101.json` contains the pre-processed high-ambiguity moralchoice dataset. Data source: [MoralChoice](https://github.com/ninodimontalcino/moralchoice).
- `moralchoice_low_ambiguity_100.json` contains the pre-processed low-ambiguity moralchoice dataset. Data source: [MoralChoice](https://github.com/ninodimontalcino/moralchoice).
- `socialchemistry_morality_ethics_100.json` contains the pre-processed socialchemistry dataset. Data source: [Social Chemistry 101](https://maxwellforbes.com/social-chemistry/).
<!-- , including five moral dimensions. `general_capability` contains data to evaluate general capability before and after model editing. -->


### Running Experiments

**Run example**: To get started (e.g. using ROME to edit llama3-8b on the places_landmark data), run:

```bash
cd ./code
python3 edit_all_method.py \
    --model_name=llama3-8b \
    --edit_method=ROME \
    --topic_name=places_landmark \
    --device_edit=0 \
    --device_eval=1 \
    --data_size=5 \
    --results_dir=../new_results_dir \
    --question_types rephrase_questions questions_2hop
```

Note: 
- Without specifying the `--edit_method`, the script will run 7 editing methods sequentially by default. 
- Specify `--question_types` to choose specific types of questions in the evaluation (The example above will only evalute 2-hop questions and rephrased questions). Otherwise, the script will run all the question types (yes_questions, no_questions, locality_questions, rephrase_questions, multiple_choice_questions, reversed_relation_questions, questions_2hop, questions_3hop, questions_4hop, questions_5hop, questions_6hop). The original questions is always included.
- Specify `--results_dir` to save the results to a specific directory, otherwise the default directory is where we save the results that we report in the paper. You can also use `--overwrite_result` to overwrite the existing result file.
<!-- If you use an API model (such as GPT-4) as the evaluator, you need to set your `YOUR_API_KEY` in Line 60 of `code/editor_new_eval.py`. One example is as follows: -->

To run the multi-turn editing, here is an example:
```bash
python3 edit_all_method_multi_turn.py \
    --model_name=llama3-8b \
    --edit_method=ROME \
    --topic_name=places_landmark \
    --device_edit=0 \
    --device_eval=1 \
    --model_eval=meta-llama/Meta-Llama-3-8B-Instruct \
    --data_size=5 \
    --results_dir=../new_results_dir \
    --multi_turn=yes \
    --multi_turn_num=10
```
- Use `--multi_turn` to choose the type of multi-turn evaluation (`yes` or `sure`).
- Use `--multi_turn_num` to set the number of turns for multi-turn evaluation.


We use a local LLM (e.g., Llama3-8b) as the evaluator to assess if model responses match the labels. For experiments, we recommend using at least one GPU with 48 GB of memory (e.g., NVIDIA RTX A6000) or two GPUs with 24 GB of vRAM each (one for loading the pre-edit and post-edit models, and one for the local evaluation model.) Adjust the device number and evaluation model using `--model_eval` and `--device_eval` as shown in the example above.

For full experiments to reproduce the results in the paper:
1. Experiment for all the 26 topics:
    ```bash
    ./edit_all_topic.sh
    ```

2. Experiment for the robustness evaluation:
    ```bash
    ./code/edit_all_topic_multi_turn.sh
    ```


We evaluate models including `Llama-2-7B-chat`, `Llama-3-8B-Instruct`, and `Mistral-7B-v0.3`. All parameters are in the `code/hparams/<method_name>/<model_name>`. 

Results are stored at `llama_2_7b_chat_hf`, `meta_llama_3_8b_instruct`, `mistral_7b_instruct_v0.3` under the `results` folder.

To summarize the results, use the jupyter notebook `code/result_table.ipynb`


## Acknowledgements
We gratefully acknowledge the use of code and data from the following projects: [ETHICS](https://github.com/hendrycks/ethics), [Jiminy Cricket](https://huggingface.co/datasets/AI-Secure/DecodingTrust/tree/main/machine_ethics), [MoralChoice](https://github.com/ninodimontalcino/moralchoice), [Social Chemistry 101](https://maxwellforbes.com/social-chemistry/), [GSM8K](https://github.com/openai/gsm8k), [BoolQ](https://github.com/google-research-datasets/boolean-questions), [NLI](https://github.com/hendrycks/nli), [Natural Questions](https://github.com/google-research-datasets/natural-questions), [GRACE](https://github.com/thartvigsen/grace), [EasyEdit](https://github.com/zjunlp/EasyEdit), [ROME](https://github.com/kmeng01/rome), [MEMIT](https://github.com/kmeng01/memit).


## Citation
```
@inproceedings{huang2025,

}
```
